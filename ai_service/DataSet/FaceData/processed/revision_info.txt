arguments: C:\Users\Admin\Downloads\facenet-ft-mtcnn-for-exam-web\src\align_dataset_mtcnn.py C:\Users\Admin\Downloads\facenet-ft-mtcnn-for-exam-web\Dataset\FaceData\raw\obama C:\Users\Admin\Downloads\facenet-ft-mtcnn-for-exam-web\Dataset\FaceData\processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 2.19.0
--------------------
git hash: b'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19'
--------------------
b'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\nindex d3829c7..7255ab1 100644\n--- a/DataSet/FaceData/processed/revision_info.txt\n+++ b/DataSet/FaceData/processed/revision_info.txt\n@@ -1,7 +1,7 @@\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\n+arguments: C:\\Users\\Admin\\Downloads\\facenet-ft-mtcnn-for-exam-web\\src\\align_dataset_mtcnn.py C:\\Users\\Admin\\Downloads\\facenet-ft-mtcnn-for-exam-web\\Dataset\\FaceData\\raw\\obama C:\\Users\\Admin\\Downloads\\facenet-ft-mtcnn-for-exam-web\\Dataset\\FaceData\\processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\n --------------------\n-tensorflow version: 2.13.0\n+tensorflow version: 2.19.0\n --------------------\n-git hash: b\'69ff1e149c0d84a123d6516ddd82970e65392608\'\n+git hash: b\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\'\n --------------------\n-b\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\nindex 2300ff3..51ec24b 100644\\n--- a/src/align/detect_face.py\\n+++ b/src/align/detect_face.py\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\n \\n import numpy as np\\n import tensorflow as tf\\n-#from math import floor\\n import cv2\\n import os\\n \\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\n #         for a2 in range(0,ws):\\n #             for a3 in range(0,3):\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\n-#     return im_data\\n-\\n+#     return im_data\\n\\\\ No newline at end of file\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\nindex 1a425a5..b0bc64f 100644\\n--- a/src/face_rec_cam.py\\n+++ b/src/face_rec_cam.py\\n@@ -1,11 +1,5 @@\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n import tensorflow as tf\\n from imutils.video import VideoStream\\n-\\n-\\n import argparse\\n import facenet\\n import imutils\\n@@ -19,7 +13,6 @@ import cv2\\n import collections\\n from sklearn.svm import SVC\\n \\n-\\n def main():\\n     parser = argparse.ArgumentParser()\\n     parser.add_argument(\\\'--path\\\', help=\\\'Path of the video you want to test on.\\\', default=0)\\n@@ -39,43 +32,44 @@ def main():\\n         model, class_names = pickle.load(file)\\n     print("Custom Classifier, Successfully loaded")\\n \\n-    with tf.Graph().as_default():\\n-\\n-        # Cai dat GPU neu co\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n+    # Use TensorFlow 2.x features\\n+    physical_devices = tf.config.list_physical_devices(\\\'GPU\\\')\\n+    if physical_devices:\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\n \\n-        with sess.as_default():\\n-\\n-            # Load the model\\n+    # Create the session and load the model\\n+    with tf.Graph().as_default():\\n+        with tf.compat.v1.Session() as sess:\\n+            # Load the facenet model\\n             print(\\\'Loading feature extraction model\\\')\\n             facenet.load_model(FACENET_MODEL_PATH)\\n \\n             # Get input and output tensors\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n-            embedding_size = embeddings.get_shape()[1]\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\n+            embedding_size = embeddings.shape[1]\\n \\n+            # Initialize MTCNN\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\n \\n             people_detected = set()\\n             person_detected = collections.Counter()\\n \\n-            cap  = VideoStream(src=0).start()\\n+            cap = VideoStream(src=0).start()\\n \\n-            while (True):\\n+            while True:\\n                 frame = cap.read()\\n                 frame = imutils.resize(frame, width=600)\\n                 frame = cv2.flip(frame, 1)\\n \\n+                # Detect faces\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\n \\n                 faces_found = bounding_boxes.shape[0]\\n                 try:\\n                     if faces_found > 1:\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\n                     elif faces_found > 0:\\n                         det = bounding_boxes[:, 0:4]\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\n@@ -84,27 +78,27 @@ def main():\\n                             bb[i][1] = det[i][1]\\n                             bb[i][2] = det[i][2]\\n                             bb[i][3] = det[i][3]\\n-                            print(bb[i][3]-bb[i][1])\\n+                            print(bb[i][3] - bb[i][1])\\n                             print(frame.shape[0])\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\n-                                                    interpolation=cv2.INTER_CUBIC)\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\n                                 scaled = facenet.prewhiten(scaled)\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\n+\\n+                                # Run the model to get embeddings\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\n \\n+                                # Predict the class of the detected face\\n                                 predictions = model.predict_proba(emb_array)\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\n-                                best_class_probabilities = predictions[\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\n                                 best_name = class_names[best_class_indices[0]]\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\n-\\n-\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\n \\n+                                # If confidence is high enough, label the face\\n                                 if best_class_probabilities > 0.8:\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\n                                     text_x = bb[i][0]\\n@@ -120,7 +114,8 @@ def main():\\n                                 else:\\n                                     name = "Unknown"\\n \\n-                except:\\n+                except Exception as e:\\n+                    print(e)\\n                     pass\\n \\n                 cv2.imshow(\\\'Face Recognition\\\', frame)\\n@@ -130,5 +125,5 @@ def main():\\n             cap.release()\\n             cv2.destroyAllWindows()\\n \\n-\\n-main()\\n+if __name__ == \\\'__main__\\\':\\n+    main()\\ndiff --git a/src/facenet.py b/src/facenet.py\\nindex 26a4e3d..2b89fbc 100644\\n--- a/src/facenet.py\\n+++ b/src/facenet.py\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\n     return dataset\\n \\n def get_image_paths(facedir):\\n+    valid_extensions = [\\\'.jpg\\\', \\\'.png\\\']  # Th\\xc3\\xaam c\\xc3\\xa1c \\xc4\\x91\\xe1\\xbb\\x8bnh d\\xe1\\xba\\xa1ng kh\\xc3\\xa1c n\\xe1\\xba\\xbfu c\\xe1\\xba\\xa7n\\n     image_paths = []\\n     if os.path.isdir(facedir):\\n         images = os.listdir(facedir)\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\n+        image_paths = [os.path.join(facedir, img) for img in images \\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\n     return image_paths\\n+\\n   \\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\n     if mode==\\\'SPLIT_CLASSES\\\':\'\n\\ No newline at end of file\n+b\'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\\nindex d3829c7..165bd85 100644\\n--- a/DataSet/FaceData/processed/revision_info.txt\\n+++ b/DataSet/FaceData/processed/revision_info.txt\\n@@ -1,7 +1,7 @@\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\n+arguments: C:\\\\Users\\\\Admin\\\\Downloads\\\\facenet-ft-mtcnn-for-exam-web\\\\src\\\\align_dataset_mtcnn.py C:\\\\Users\\\\Admin\\\\Downloads\\\\facenet-ft-mtcnn-for-exam-web\\\\Dataset\\\\FaceData\\\\raw\\\\obama C:\\\\Users\\\\Admin\\\\Downloads\\\\facenet-ft-mtcnn-for-exam-web\\\\Dataset\\\\FaceData\\\\processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\n --------------------\\n-tensorflow version: 2.13.0\\n+tensorflow version: 2.19.0\\n --------------------\\n-git hash: b\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\'\\n+git hash: b\\\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\\\'\\n --------------------\\n-b\\\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\\\nindex 2300ff3..51ec24b 100644\\\\n--- a/src/align/detect_face.py\\\\n+++ b/src/align/detect_face.py\\\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\\\n \\\\n import numpy as np\\\\n import tensorflow as tf\\\\n-#from math import floor\\\\n import cv2\\\\n import os\\\\n \\\\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\\\n #         for a2 in range(0,ws):\\\\n #             for a3 in range(0,3):\\\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\n-#     return im_data\\\\n-\\\\n+#     return im_data\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\nindex 1a425a5..b0bc64f 100644\\\\n--- a/src/face_rec_cam.py\\\\n+++ b/src/face_rec_cam.py\\\\n@@ -1,11 +1,5 @@\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n import tensorflow as tf\\\\n from imutils.video import VideoStream\\\\n-\\\\n-\\\\n import argparse\\\\n import facenet\\\\n import imutils\\\\n@@ -19,7 +13,6 @@ import cv2\\\\n import collections\\\\n from sklearn.svm import SVC\\\\n \\\\n-\\\\n def main():\\\\n     parser = argparse.ArgumentParser()\\\\n     parser.add_argument(\\\\\\\'--path\\\\\\\', help=\\\\\\\'Path of the video you want to test on.\\\\\\\', default=0)\\\\n@@ -39,43 +32,44 @@ def main():\\\\n         model, class_names = pickle.load(file)\\\\n     print("Custom Classifier, Successfully loaded")\\\\n \\\\n-    with tf.Graph().as_default():\\\\n-\\\\n-        # Cai dat GPU neu co\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n+    # Use TensorFlow 2.x features\\\\n+    physical_devices = tf.config.list_physical_devices(\\\\\\\'GPU\\\\\\\')\\\\n+    if physical_devices:\\\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\n \\\\n-        with sess.as_default():\\\\n-\\\\n-            # Load the model\\\\n+    # Create the session and load the model\\\\n+    with tf.Graph().as_default():\\\\n+        with tf.compat.v1.Session() as sess:\\\\n+            # Load the facenet model\\\\n             print(\\\\\\\'Loading feature extraction model\\\\\\\')\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\n \\\\n             # Get input and output tensors\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n+            embedding_size = embeddings.shape[1]\\\\n \\\\n+            # Initialize MTCNN\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\n \\\\n             people_detected = set()\\\\n             person_detected = collections.Counter()\\\\n \\\\n-            cap  = VideoStream(src=0).start()\\\\n+            cap = VideoStream(src=0).start()\\\\n \\\\n-            while (True):\\\\n+            while True:\\\\n                 frame = cap.read()\\\\n                 frame = imutils.resize(frame, width=600)\\\\n                 frame = cv2.flip(frame, 1)\\\\n \\\\n+                # Detect faces\\\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\n \\\\n                 faces_found = bounding_boxes.shape[0]\\\\n                 try:\\\\n                     if faces_found > 1:\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\n                     elif faces_found > 0:\\\\n                         det = bounding_boxes[:, 0:4]\\\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\n@@ -84,27 +78,27 @@ def main():\\\\n                             bb[i][1] = det[i][1]\\\\n                             bb[i][2] = det[i][2]\\\\n                             bb[i][3] = det[i][3]\\\\n-                            print(bb[i][3]-bb[i][1])\\\\n+                            print(bb[i][3] - bb[i][1])\\\\n                             print(frame.shape[0])\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\n                                 scaled = facenet.prewhiten(scaled)\\\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\n+\\\\n+                                # Run the model to get embeddings\\\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\n \\\\n+                                # Predict the class of the detected face\\\\n                                 predictions = model.predict_proba(emb_array)\\\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\\\n-                                best_class_probabilities = predictions[\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\n                                 best_name = class_names[best_class_indices[0]]\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\n-\\\\n-\\\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\n \\\\n+                                # If confidence is high enough, label the face\\\\n                                 if best_class_probabilities > 0.8:\\\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\n                                     text_x = bb[i][0]\\\\n@@ -120,7 +114,8 @@ def main():\\\\n                                 else:\\\\n                                     name = "Unknown"\\\\n \\\\n-                except:\\\\n+                except Exception as e:\\\\n+                    print(e)\\\\n                     pass\\\\n \\\\n                 cv2.imshow(\\\\\\\'Face Recognition\\\\\\\', frame)\\\\n@@ -130,5 +125,5 @@ def main():\\\\n             cap.release()\\\\n             cv2.destroyAllWindows()\\\\n \\\\n-\\\\n-main()\\\\n+if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n+    main()\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\nindex 26a4e3d..2b89fbc 100644\\\\n--- a/src/facenet.py\\\\n+++ b/src/facenet.py\\\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\\\n     return dataset\\\\n \\\\n def get_image_paths(facedir):\\\\n+    valid_extensions = [\\\\\\\'.jpg\\\\\\\', \\\\\\\'.png\\\\\\\']  # Th\\\\xc3\\\\xaam c\\\\xc3\\\\xa1c \\\\xc4\\\\x91\\\\xe1\\\\xbb\\\\x8bnh d\\\\xe1\\\\xba\\\\xa1ng kh\\\\xc3\\\\xa1c n\\\\xe1\\\\xba\\\\xbfu c\\\\xe1\\\\xba\\\\xa7n\\\\n     image_paths = []\\\\n     if os.path.isdir(facedir):\\\\n         images = os.listdir(facedir)\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\n+        image_paths = [os.path.join(facedir, img) for img in images \\\\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\\\n     return image_paths\\\\n+\\\\n   \\\\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\n     if mode==\\\\\\\'SPLIT_CLASSES\\\\\\\':\\\'\\n\\\\ No newline at end of file\\n+b\\\'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\\\\nindex d3829c7..0660cbe 100644\\\\n--- a/DataSet/FaceData/processed/revision_info.txt\\\\n+++ b/DataSet/FaceData/processed/revision_info.txt\\\\n@@ -1,7 +1,7 @@\\\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\n+arguments: C:\\\\\\\\Users\\\\\\\\Admin\\\\\\\\Downloads\\\\\\\\facenet-ft-mtcnn-for-exam-web\\\\\\\\src\\\\\\\\align_dataset_mtcnn.py C:\\\\\\\\Users\\\\\\\\Admin\\\\\\\\Downloads\\\\\\\\facenet-ft-mtcnn-for-exam-web\\\\\\\\Dataset\\\\\\\\FaceData\\\\\\\\raw\\\\\\\\obama C:\\\\\\\\Users\\\\\\\\Admin\\\\\\\\Downloads\\\\\\\\facenet-ft-mtcnn-for-exam-web\\\\\\\\Dataset\\\\\\\\FaceData\\\\\\\\processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\n --------------------\\\\n-tensorflow version: 2.13.0\\\\n+tensorflow version: 2.19.0\\\\n --------------------\\\\n-git hash: b\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\'\\\\n+git hash: b\\\\\\\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\\\\\\\'\\\\n --------------------\\\\n-b\\\\\\\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\\\\\\\nindex 2300ff3..51ec24b 100644\\\\\\\\n--- a/src/align/detect_face.py\\\\\\\\n+++ b/src/align/detect_face.py\\\\\\\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\\\\\\\n \\\\\\\\n import numpy as np\\\\\\\\n import tensorflow as tf\\\\\\\\n-#from math import floor\\\\\\\\n import cv2\\\\\\\\n import os\\\\\\\\n \\\\\\\\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\\\\\\\n #         for a2 in range(0,ws):\\\\\\\\n #             for a3 in range(0,3):\\\\\\\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\\\\\n-#     return im_data\\\\\\\\n-\\\\\\\\n+#     return im_data\\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\nindex 1a425a5..b0bc64f 100644\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\n@@ -1,11 +1,5 @@\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n import tensorflow as tf\\\\\\\\n from imutils.video import VideoStream\\\\\\\\n-\\\\\\\\n-\\\\\\\\n import argparse\\\\\\\\n import facenet\\\\\\\\n import imutils\\\\\\\\n@@ -19,7 +13,6 @@ import cv2\\\\\\\\n import collections\\\\\\\\n from sklearn.svm import SVC\\\\\\\\n \\\\\\\\n-\\\\\\\\n def main():\\\\\\\\n     parser = argparse.ArgumentParser()\\\\\\\\n     parser.add_argument(\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\', default=0)\\\\\\\\n@@ -39,43 +32,44 @@ def main():\\\\\\\\n         model, class_names = pickle.load(file)\\\\\\\\n     print("Custom Classifier, Successfully loaded")\\\\\\\\n \\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n+    # Use TensorFlow 2.x features\\\\\\\\n+    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\')\\\\\\\\n+    if physical_devices:\\\\\\\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\n \\\\\\\\n-        with sess.as_default():\\\\\\\\n-\\\\\\\\n-            # Load the model\\\\\\\\n+    # Create the session and load the model\\\\\\\\n+    with tf.Graph().as_default():\\\\\\\\n+        with tf.compat.v1.Session() as sess:\\\\\\\\n+            # Load the facenet model\\\\\\\\n             print(\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\')\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n \\\\\\\\n             # Get input and output tensors\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n+            embedding_size = embeddings.shape[1]\\\\\\\\n \\\\\\\\n+            # Initialize MTCNN\\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\n \\\\\\\\n             people_detected = set()\\\\\\\\n             person_detected = collections.Counter()\\\\\\\\n \\\\\\\\n-            cap  = VideoStream(src=0).start()\\\\\\\\n+            cap = VideoStream(src=0).start()\\\\\\\\n \\\\\\\\n-            while (True):\\\\\\\\n+            while True:\\\\\\\\n                 frame = cap.read()\\\\\\\\n                 frame = imutils.resize(frame, width=600)\\\\\\\\n                 frame = cv2.flip(frame, 1)\\\\\\\\n \\\\\\\\n+                # Detect faces\\\\\\\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\n \\\\\\\\n                 faces_found = bounding_boxes.shape[0]\\\\\\\\n                 try:\\\\\\\\n                     if faces_found > 1:\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n                     elif faces_found > 0:\\\\\\\\n                         det = bounding_boxes[:, 0:4]\\\\\\\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\n@@ -84,27 +78,27 @@ def main():\\\\\\\\n                             bb[i][1] = det[i][1]\\\\\\\\n                             bb[i][2] = det[i][2]\\\\\\\\n                             bb[i][3] = det[i][3]\\\\\\\\n-                            print(bb[i][3]-bb[i][1])\\\\\\\\n+                            print(bb[i][3] - bb[i][1])\\\\\\\\n                             print(frame.shape[0])\\\\\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\\\\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\n                                 scaled = facenet.prewhiten(scaled)\\\\\\\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\n+\\\\\\\\n+                                # Run the model to get embeddings\\\\\\\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n \\\\\\\\n+                                # Predict the class of the detected face\\\\\\\\n                                 predictions = model.predict_proba(emb_array)\\\\\\\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\n-                                best_class_probabilities = predictions[\\\\\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n                                 best_name = class_names[best_class_indices[0]]\\\\\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\n-\\\\\\\\n-\\\\\\\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\n \\\\\\\\n+                                # If confidence is high enough, label the face\\\\\\\\n                                 if best_class_probabilities > 0.8:\\\\\\\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\n                                     text_x = bb[i][0]\\\\\\\\n@@ -120,7 +114,8 @@ def main():\\\\\\\\n                                 else:\\\\\\\\n                                     name = "Unknown"\\\\\\\\n \\\\\\\\n-                except:\\\\\\\\n+                except Exception as e:\\\\\\\\n+                    print(e)\\\\\\\\n                     pass\\\\\\\\n \\\\\\\\n                 cv2.imshow(\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\', frame)\\\\\\\\n@@ -130,5 +125,5 @@ def main():\\\\\\\\n             cap.release()\\\\\\\\n             cv2.destroyAllWindows()\\\\\\\\n \\\\\\\\n-\\\\\\\\n-main()\\\\\\\\n+if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n+    main()\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\nindex 26a4e3d..2b89fbc 100644\\\\\\\\n--- a/src/facenet.py\\\\\\\\n+++ b/src/facenet.py\\\\\\\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\\\\\\\n     return dataset\\\\\\\\n \\\\\\\\n def get_image_paths(facedir):\\\\\\\\n+    valid_extensions = [\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\']  # Th\\\\\\\\xc3\\\\\\\\xaam c\\\\\\\\xc3\\\\\\\\xa1c \\\\\\\\xc4\\\\\\\\x91\\\\\\\\xe1\\\\\\\\xbb\\\\\\\\x8bnh d\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xa1ng kh\\\\\\\\xc3\\\\\\\\xa1c n\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xbfu c\\\\\\\\xe1\\\\\\\\xba\\\\\\\\xa7n\\\\\\\\n     image_paths = []\\\\\\\\n     if os.path.isdir(facedir):\\\\\\\\n         images = os.listdir(facedir)\\\\\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\\\\\n+        image_paths = [os.path.join(facedir, img) for img in images \\\\\\\\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\\\\\\\n     return image_paths\\\\\\\\n+\\\\\\\\n   \\\\\\\\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\\\\\n     if mode==\\\\\\\\\\\\\\\'SPLIT_CLASSES\\\\\\\\\\\\\\\':\\\\\\\'\\\\n\\\\\\\\ No newline at end of file\\\\n+b\\\\\\\'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\nindex d3829c7..62a11a3 100644\\\\\\\\n--- a/DataSet/FaceData/processed/revision_info.txt\\\\\\\\n+++ b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\n@@ -1,7 +1,7 @@\\\\\\\\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\n+arguments: align_dataset_mtcnn.py C:\\\\\\\\\\\\\\\\Users\\\\\\\\\\\\\\\\Admin\\\\\\\\\\\\\\\\Downloads\\\\\\\\\\\\\\\\facenet-ft-mtcnn-for-exam-web\\\\\\\\\\\\\\\\Dataset\\\\\\\\\\\\\\\\FaceData\\\\\\\\\\\\\\\\raw\\\\\\\\\\\\\\\\nklam C:\\\\\\\\\\\\\\\\Users\\\\\\\\\\\\\\\\Admin\\\\\\\\\\\\\\\\Downloads\\\\\\\\\\\\\\\\facenet-ft-mtcnn-for-exam-web\\\\\\\\\\\\\\\\Dataset\\\\\\\\\\\\\\\\FaceData\\\\\\\\\\\\\\\\processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\n --------------------\\\\\\\\n-tensorflow version: 2.13.0\\\\\\\\n+tensorflow version: 2.19.0\\\\\\\\n --------------------\\\\\\\\n-git hash: b\\\\\\\\\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\\\\\\\\\'\\\\\\\\n+git hash: b\\\\\\\\\\\\\\\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\\\\\\\\\\\\\\\'\\\\\\\\n --------------------\\\\\\\\n-b\\\\\\\\\\\\\\\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\\\\\\\\\\\\\\\nindex 2300ff3..51ec24b 100644\\\\\\\\\\\\\\\\n--- a/src/align/detect_face.py\\\\\\\\\\\\\\\\n+++ b/src/align/detect_face.py\\\\\\\\\\\\\\\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n import numpy as np\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\n-#from math import floor\\\\\\\\\\\\\\\\n import cv2\\\\\\\\\\\\\\\\n import os\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\\\\\\\\\\\\\\\n #         for a2 in range(0,ws):\\\\\\\\\\\\\\\\n #             for a3 in range(0,3):\\\\\\\\\\\\\\\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\\\\\\\\\\\\\n-#     return im_data\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n+#     return im_data\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\nindex 1a425a5..b0bc64f 100644\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\\\\\\\\\n@@ -1,11 +1,5 @@\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\n from imutils.video import VideoStream\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n import argparse\\\\\\\\\\\\\\\\n import facenet\\\\\\\\\\\\\\\\n import imutils\\\\\\\\\\\\\\\\n@@ -19,7 +13,6 @@ import cv2\\\\\\\\\\\\\\\\n import collections\\\\\\\\\\\\\\\\n from sklearn.svm import SVC\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n def main():\\\\\\\\\\\\\\\\n     parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n     parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n@@ -39,43 +32,44 @@ def main():\\\\\\\\\\\\\\\\n         model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\n     print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\n+    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\n+    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n+    if physical_devices:\\\\\\\\\\\\\\\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Load the model\\\\\\\\\\\\\\\\n+    # Create the session and load the model\\\\\\\\\\\\\\\\n+    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n+        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\n+            # Load the facenet model\\\\\\\\\\\\\\\\n             print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n             # Get input and output tensors\\\\\\\\\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n+            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n+            # Initialize MTCNN\\\\\\\\\\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n             people_detected = set()\\\\\\\\\\\\\\\\n             person_detected = collections.Counter()\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-            cap  = VideoStream(src=0).start()\\\\\\\\\\\\\\\\n+            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-            while (True):\\\\\\\\\\\\\\\\n+            while True:\\\\\\\\\\\\\\\\n                 frame = cap.read()\\\\\\\\\\\\\\\\n                 frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\n                 frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n+                # Detect faces\\\\\\\\\\\\\\\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n                 faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\n                 try:\\\\\\\\\\\\\\\\n                     if faces_found > 1:\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n                     elif faces_found > 0:\\\\\\\\\\\\\\\\n                         det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\n@@ -84,27 +78,27 @@ def main():\\\\\\\\\\\\\\\\n                             bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\n                             bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\n                             bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\n-                            print(bb[i][3]-bb[i][1])\\\\\\\\\\\\\\\\n+                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\n                             print(frame.shape[0])\\\\\\\\\\\\\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\\\\\\\\\\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\\\\\\\\\\\\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\n                                 scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\n+                                # Run the model to get embeddings\\\\\\\\\\\\\\\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n+                                # Predict the class of the detected face\\\\\\\\\\\\\\\\n                                 predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\n                                 best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n+                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\n                                 if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\n                                     text_x = bb[i][0]\\\\\\\\\\\\\\\\n@@ -120,7 +114,8 @@ def main():\\\\\\\\\\\\\\\\n                                 else:\\\\\\\\\\\\\\\\n                                     name = "Unknown"\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-                except:\\\\\\\\\\\\\\\\n+                except Exception as e:\\\\\\\\\\\\\\\\n+                    print(e)\\\\\\\\\\\\\\\\n                     pass\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n                 cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\n@@ -130,5 +125,5 @@ def main():\\\\\\\\\\\\\\\\n             cap.release()\\\\\\\\\\\\\\\\n             cv2.destroyAllWindows()\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-main()\\\\\\\\\\\\\\\\n+if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n+    main()\\\\\\\\\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\\\\\\\\\nindex 26a4e3d..2b89fbc 100644\\\\\\\\\\\\\\\\n--- a/src/facenet.py\\\\\\\\\\\\\\\\n+++ b/src/facenet.py\\\\\\\\\\\\\\\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\\\\\\\\\\\\\\\n     return dataset\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\n def get_image_paths(facedir):\\\\\\\\\\\\\\\\n+    valid_extensions = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']  # Th\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xaam c\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xa1c \\\\\\\\\\\\\\\\xc4\\\\\\\\\\\\\\\\x91\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xbb\\\\\\\\\\\\\\\\x8bnh d\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xa1ng kh\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\xa1c n\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xbfu c\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\xa7n\\\\\\\\\\\\\\\\n     image_paths = []\\\\\\\\\\\\\\\\n     if os.path.isdir(facedir):\\\\\\\\\\\\\\\\n         images = os.listdir(facedir)\\\\\\\\\\\\\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\\\\\\\\\\\\\n+        image_paths = [os.path.join(facedir, img) for img in images \\\\\\\\\\\\\\\\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\\\\\\\\\\\\\\\n     return image_paths\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\n   \\\\\\\\\\\\\\\\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\\\\\\\\\\\\\n     if mode==\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'SPLIT_CLASSES\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\'\\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\n+b\\\\\\\\\\\\\\\'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\nindex d3829c7..39b90e3 100644\\\\\\\\\\\\\\\\n--- a/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\n+++ b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\n@@ -1,7 +1,7 @@\\\\\\\\\\\\\\\\n arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\n-tensorflow version: 2.13.0\\\\\\\\\\\\\\\\n+tensorflow version: 2.18.0\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\n-git hash: b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n+git hash: b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\n-b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 2300ff3..51ec24b 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-#from math import floor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #         for a2 in range(0,ws):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #             for a3 in range(0,3):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-#     return im_data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+#     return im_data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 1a425a5..b0bc64f 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,11 +1,5 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n from imutils.video import VideoStream\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import imutils\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -19,7 +13,6 @@ import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -39,43 +32,44 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    if physical_devices:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    # Create the session and load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            # Load the facenet model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             # Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            # Initialize MTCNN\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             people_detected = set()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             person_detected = collections.Counter()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap  = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            while (True):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            while True:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = cap.read()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                # Detect faces\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 try:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     if faces_found > 1:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     elif faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                         det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -84,27 +78,27 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(bb[i][3]-bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             print(frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # Run the model to get embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # Predict the class of the detected face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     text_x = bb[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -120,7 +114,8 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                except:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                except Exception as e:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                    print(e)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     pass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -130,5 +125,5 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             cap.release()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             cv2.destroyAllWindows()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 26a4e3d..2b89fbc 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     return dataset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def get_image_paths(facedir):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    valid_extensions = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']  # Th\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xaam c\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1c \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc4\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\x91\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xbb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\x8bnh d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1ng kh\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1c n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xbfu c\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa7n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     image_paths = []\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if os.path.isdir(facedir):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         images = os.listdir(facedir)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        image_paths = [os.path.join(facedir, img) for img in images \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     return image_paths\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if mode==\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'SPLIT_CLASSES\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\n+b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex d3829c7..a3ac99e 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,7 +1,7 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-tensorflow version: 2.13.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+tensorflow version: 2.18.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-git hash: b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+git hash: b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 2300ff3..51ec24b 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-#from math import floor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #         for a2 in range(0,ws):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #             for a3 in range(0,3):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-#     return im_data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+#     return im_data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 1a425a5..b0bc64f 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,11 +1,5 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n from imutils.video import VideoStream\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import imutils\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -19,7 +13,6 @@ import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -39,43 +32,44 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    if physical_devices:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    # Create the session and load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            # Load the facenet model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             # Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            # Initialize MTCNN\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             people_detected = set()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             person_detected = collections.Counter()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap  = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            while (True):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            while True:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = cap.read()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                # Detect faces\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 try:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     if faces_found > 1:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     elif faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                         det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -84,27 +78,27 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(bb[i][3]-bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             print(frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # Run the model to get embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # Predict the class of the detected face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     text_x = bb[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -120,7 +114,8 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                except:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                except Exception as e:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                    print(e)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     pass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -130,5 +125,5 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             cap.release()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             cv2.destroyAllWindows()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 26a4e3d..2b89fbc 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     return dataset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def get_image_paths(facedir):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    valid_extensions = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']  # Th\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xaam c\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1c \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc4\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\x91\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xbb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\x8bnh d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1ng kh\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1c n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xbfu c\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa7n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     image_paths = []\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if os.path.isdir(facedir):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         images = os.listdir(facedir)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        image_paths = [os.path.join(facedir, img) for img in images \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     return image_paths\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if mode==\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'SPLIT_CLASSES\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'diff --git a/DataSet/FaceData/processed/revision_info.txt b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex d3829c7..491faac 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/DataSet/FaceData/processed/revision_info.txt\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,7 +1,7 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-tensorflow version: 2.13.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+tensorflow version: 2.18.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-git hash: b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'69ff1e149c0d84a123d6516ddd82970e65392608\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+git hash: b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'89fc7dc33bb7545eff888dd93d1e1cbc1b6eaf19\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n --------------------\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'diff --git a/src/align/detect_face.py b/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 2300ff3..51ec24b 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/align/detect_face.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -30,7 +30,6 @@ from six import string_types, iteritems\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-#from math import floor\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -777,5 +776,4 @@ def imresample(img, sz):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #         for a2 in range(0,ws):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #             for a3 in range(0,3):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n #                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-#     return im_data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+#     return im_data\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 1a425a5..b0bc64f 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,11 +1,5 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n from imutils.video import VideoStream\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import imutils\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -19,7 +13,6 @@ import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -39,43 +32,44 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        # Cai dat GPU neu co\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    if physical_devices:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with sess.as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    # Create the session and load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            # Load the facenet model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             # Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            # Initialize MTCNN\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             people_detected = set()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             person_detected = collections.Counter()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap  = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            while (True):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+            while True:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = cap.read()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                # Detect faces\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 try:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     if faces_found > 1:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     elif faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                         det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                         bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -84,27 +78,27 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(bb[i][3]-bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                             print(frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # Run the model to get embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # Predict the class of the detected face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     text_x = bb[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -120,7 +114,8 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                 else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                                     name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                except:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                except Exception as e:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                    print(e)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                     pass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n                 cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -130,5 +125,5 @@ def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             cap.release()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             cv2.destroyAllWindows()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 26a4e3d..2b89fbc 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -331,11 +331,14 @@ def get_dataset(path, has_class_directories=True):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     return dataset\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def get_image_paths(facedir):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+    valid_extensions = [\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.jpg\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'.png\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\']  # Th\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xaam c\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1c \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc4\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\x91\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xbb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\x8bnh d\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1ng kh\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xc3\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa1c n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xbfu c\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xe1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xba\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\xa7n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     image_paths = []\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if os.path.isdir(facedir):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         images = os.listdir(facedir)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        image_paths = [os.path.join(facedir,img) for img in images]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        image_paths = [os.path.join(facedir, img) for img in images \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+                       if os.path.splitext(img)[1].lower() in valid_extensions]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     return image_paths\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n   \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if mode==\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'SPLIT_CLASSES\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+b\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'diff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex b0bc64f..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,129 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from imutils.video import VideoStream\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import imutils\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    args = parser.parse_args()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    MINSIZE = 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    FACTOR = 0.709\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Load The Custom Classifier\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    if physical_devices:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Create the session and load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Load the facenet model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Initialize MTCNN\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            people_detected = set()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            while True:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = cap.read()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                # Detect faces\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                try:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    if faces_found > 1:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    elif faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # Run the model to get embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # Predict the class of the detected face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    text_x = bb[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    text_y = bb[i][3] + 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    person_detected[best_name] += 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                except Exception as e:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    print(e)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    pass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    break\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap.release()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex aaf7193..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_flask.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,118 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask import Flask\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask import render_template , request\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask_cors import CORS, cross_origin\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import base64\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-MINSIZE = 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-FACTOR = 0.709\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-IMAGE_SIZE = 182\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Load The Custom Classifier\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-tf.Graph().as_default()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Cai dat GPU neu co\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-app = Flask(__name__)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-CORS(app)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def index():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    return "OK!";\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/recog\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', methods=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def upload_img_file():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    if request.method == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        # base 64\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        name="Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        f = request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        w = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        h = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'h\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        decoded_string = base64.b64decode(f)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        #frame = frame.reshape(w,h,3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        if faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            for i in range(faces_found):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                cropped = frame\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        return name;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    app.run(debug=True, host=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0.0.0.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',port=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'8000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex f7b2209..a9697f7 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex b0bc64f..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,129 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from imutils.video import VideoStream\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import imutils\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    args = parser.parse_args()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    MINSIZE = 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    FACTOR = 0.709\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Load The Custom Classifier\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    if physical_devices:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Create the session and load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Load the facenet model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Initialize MTCNN\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            people_detected = set()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            while True:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = cap.read()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                # Detect faces\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                try:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    if faces_found > 1:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    elif faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # Run the model to get embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # Predict the class of the detected face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    text_x = bb[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    text_y = bb[i][3] + 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    person_detected[best_name] += 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                except Exception as e:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    print(e)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    pass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    break\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap.release()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex aaf7193..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_flask.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,118 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask import Flask\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask import render_template , request\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask_cors import CORS, cross_origin\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import base64\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-MINSIZE = 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-FACTOR = 0.709\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-IMAGE_SIZE = 182\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Load The Custom Classifier\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-tf.Graph().as_default()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Cai dat GPU neu co\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-app = Flask(__name__)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-CORS(app)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def index():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    return "OK!";\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/recog\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', methods=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def upload_img_file():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    if request.method == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        # base 64\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        name="Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        f = request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        w = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        h = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'h\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        decoded_string = base64.b64decode(f)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        #frame = frame.reshape(w,h,3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        if faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            for i in range(faces_found):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                cropped = frame\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        return name;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    app.run(debug=True, host=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0.0.0.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',port=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'8000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/video/a b/video/a\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 8b13789..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/video/a\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex a503c89..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex f7b2209..a9697f7 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex b0bc64f..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,129 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from imutils.video import VideoStream\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import imutils\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def main():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    args = parser.parse_args()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    MINSIZE = 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    FACTOR = 0.709\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Load The Custom Classifier\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    if physical_devices:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    # Create the session and load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Load the facenet model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            # Initialize MTCNN\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            people_detected = set()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            while True:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = cap.read()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                # Detect faces\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                try:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    if faces_found > 1:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    elif faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print(frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # Run the model to get embeddings\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # Predict the class of the detected face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    text_x = bb[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    text_y = bb[i][3] + 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    person_detected[best_name] += 1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                except Exception as e:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    print(e)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    pass\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    break\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cap.release()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    main()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex aaf7193..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/face_rec_flask.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1,118 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask import Flask\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask import render_template , request\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from flask_cors import CORS, cross_origin\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-import base64\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-MINSIZE = 20\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-FACTOR = 0.709\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-IMAGE_SIZE = 182\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Load The Custom Classifier\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-tf.Graph().as_default()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Cai dat GPU neu co\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Load the model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-# Get input and output tensors\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-app = Flask(__name__)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-CORS(app)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def index():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    return "OK!";\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/recog\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', methods=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'])\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-def upload_img_file():\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    if request.method == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        # base 64\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        name="Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        f = request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        w = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        h = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'h\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        decoded_string = base64.b64decode(f)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        #frame = frame.reshape(w,h,3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        if faces_found > 0:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-            for i in range(faces_found):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                cropped = frame\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                else:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-                    name = "Unknown"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        return name;\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-    app.run(debug=True, host=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0.0.0.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',port=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'8000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 2b89fbc..29434e0 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ b/src/facenet.py\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -371,7 +371,7 @@ def load_model(model, input_map=None):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     model_exp = os.path.expanduser(model)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n     if (os.path.isfile(model_exp)):\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n         print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model filename: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_exp)\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+        with tf.io.gfile.GFile(model_exp,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             graph_def = tf.compat.v1.GraphDef()\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             graph_def.ParseFromString(f.read())\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n             tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/video/a b/video/a\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex 8b13789..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n--- a/video/a\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nindex a503c89..0000000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\\\\\\\\\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\\\\\\\\\\\\\\\\nindex f7b2209..a9697f7 100644\\\\\\\\\\\\\\\\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\\\\\\\\\\\\\\\\ndiff --git a/src/align/__init__.py b/src/align/__init__.py\\\\\\\\\\\\\\\\nnew file mode 100644\\\\\\\\\\\\\\\\nindex 0000000..e69de29\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex b0bc64f..0000000\\\\\\\\\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,129 +0,0 @@\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-from imutils.video import VideoStream\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import imutils\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-def main():\\\\\\\\\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', default=0)\\\\\\\\\\\\\\\\n-    args = parser.parse_args()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    MINSIZE = 20\\\\\\\\\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\n-    FACTOR = 0.709\\\\\\\\\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Load The Custom Classifier\\\\\\\\\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Use TensorFlow 2.x features\\\\\\\\\\\\\\\\n-    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-    if physical_devices:\\\\\\\\\\\\\\\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-    # Create the session and load the model\\\\\\\\\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\\\\\\\\\n-        with tf.compat.v1.Session() as sess:\\\\\\\\\\\\\\\\n-            # Load the facenet model\\\\\\\\\\\\\\\\n-            print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Get input and output tensors\\\\\\\\\\\\\\\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n-            embedding_size = embeddings.shape[1]\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            # Initialize MTCNN\\\\\\\\\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            people_detected = set()\\\\\\\\\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            cap = VideoStream(src=0).start()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            while True:\\\\\\\\\\\\\\\\n-                frame = cap.read()\\\\\\\\\\\\\\\\n-                frame = imutils.resize(frame, width=600)\\\\\\\\\\\\\\\\n-                frame = cv2.flip(frame, 1)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                # Detect faces\\\\\\\\\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\n-                try:\\\\\\\\\\\\\\\\n-                    if faces_found > 1:\\\\\\\\\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n-                    elif faces_found > 0:\\\\\\\\\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\n-                            print(bb[i][3] - bb[i][1])\\\\\\\\\\\\\\\\n-                            print(frame.shape[0])\\\\\\\\\\\\\\\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\\\\\\\\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\\\\\\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                                # Run the model to get embeddings\\\\\\\\\\\\\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                                # Predict the class of the detected face\\\\\\\\\\\\\\\\n-                                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                                # If confidence is high enough, label the face\\\\\\\\\\\\\\\\n-                                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\\\\\\\\\n-                                    text_x = bb[i][0]\\\\\\\\\\\\\\\\n-                                    text_y = bb[i][3] + 20\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\\\\\\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\\\\\\\\\n-                                    person_detected[best_name] += 1\\\\\\\\\\\\\\\\n-                                else:\\\\\\\\\\\\\\\\n-                                    name = "Unknown"\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                except Exception as e:\\\\\\\\\\\\\\\\n-                    print(e)\\\\\\\\\\\\\\\\n-                    pass\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', frame)\\\\\\\\\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'):\\\\\\\\\\\\\\\\n-                    break\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-            cap.release()\\\\\\\\\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    main()\\\\\\\\\\\\\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex aaf7193..0000000\\\\\\\\\\\\\\\\n--- a/src/face_rec_flask.py\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1,118 +0,0 @@\\\\\\\\\\\\\\\\n-from __future__ import absolute_import\\\\\\\\\\\\\\\\n-from __future__ import division\\\\\\\\\\\\\\\\n-from __future__ import print_function\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-from flask import Flask\\\\\\\\\\\\\\\\n-from flask import render_template , request\\\\\\\\\\\\\\\\n-from flask_cors import CORS, cross_origin\\\\\\\\\\\\\\\\n-import tensorflow as tf\\\\\\\\\\\\\\\\n-import argparse\\\\\\\\\\\\\\\\n-import facenet\\\\\\\\\\\\\\\\n-import os\\\\\\\\\\\\\\\\n-import sys\\\\\\\\\\\\\\\\n-import math\\\\\\\\\\\\\\\\n-import pickle\\\\\\\\\\\\\\\\n-import align.detect_face\\\\\\\\\\\\\\\\n-import numpy as np\\\\\\\\\\\\\\\\n-import cv2\\\\\\\\\\\\\\\\n-import collections\\\\\\\\\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\\\\\\\\\n-import base64\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-MINSIZE = 20\\\\\\\\\\\\\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\\\\\\\\\n-FACTOR = 0.709\\\\\\\\\\\\\\\\n-IMAGE_SIZE = 182\\\\\\\\\\\\\\\\n-INPUT_IMAGE_SIZE = 160\\\\\\\\\\\\\\\\n-CLASSIFIER_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/facemodel.pkl\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-FACENET_MODEL_PATH = \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'../Models/20180402-114759.pb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# Load The Custom Classifier\\\\\\\\\\\\\\\\n-with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as file:\\\\\\\\\\\\\\\\n-    model, class_names = pickle.load(file)\\\\\\\\\\\\\\\\n-print("Custom Classifier, Successfully loaded")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-tf.Graph().as_default()\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# Cai dat GPU neu co\\\\\\\\\\\\\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\\\\\\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# Load the model\\\\\\\\\\\\\\\\n-print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-# Get input and output tensors\\\\\\\\\\\\\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\\\\\\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\\\\\\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\\\\\\\\\n-embedding_size = embeddings.get_shape()[1]\\\\\\\\\\\\\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-app = Flask(__name__)\\\\\\\\\\\\\\\\n-CORS(app)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\n-def index():\\\\\\\\\\\\\\\\n-    return "OK!";\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'/recog\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', methods=[\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'])\\\\\\\\\\\\\\\\n-@cross_origin()\\\\\\\\\\\\\\\\n-def upload_img_file():\\\\\\\\\\\\\\\\n-    if request.method == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-        # base 64\\\\\\\\\\\\\\\\n-        name="Unknown"\\\\\\\\\\\\\\\\n-        f = request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'image\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-        w = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-        h = int(request.form.get(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'h\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        decoded_string = base64.b64decode(f)\\\\\\\\\\\\\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\\\\\\\\\\\\\n-        #frame = frame.reshape(w,h,3)\\\\\\\\\\\\\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        faces_found = bounding_boxes.shape[0]\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        if faces_found > 0:\\\\\\\\\\\\\\\\n-            det = bounding_boxes[:, 0:4]\\\\\\\\\\\\\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\\\\\\\\\n-            for i in range(faces_found):\\\\\\\\\\\\\\\\n-                bb[i][0] = det[i][0]\\\\\\\\\\\\\\\\n-                bb[i][1] = det[i][1]\\\\\\\\\\\\\\\\n-                bb[i][2] = det[i][2]\\\\\\\\\\\\\\\\n-                bb[i][3] = det[i][3]\\\\\\\\\\\\\\\\n-                cropped = frame\\\\\\\\\\\\\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\\\\\\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\\\\\\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\\\\\\\\\n-                scaled = facenet.prewhiten(scaled)\\\\\\\\\\\\\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\\\\\\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\\\\\\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\\\\\\\\\n-                predictions = model.predict_proba(emb_array)\\\\\\\\\\\\\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\\\\\\\\\n-                best_class_probabilities = predictions[\\\\\\\\\\\\\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\\\\\\\\\n-                best_name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-                if best_class_probabilities > 0.8:\\\\\\\\\\\\\\\\n-                    name = class_names[best_class_indices[0]]\\\\\\\\\\\\\\\\n-                else:\\\\\\\\\\\\\\\\n-                    name = "Unknown"\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-        return name;\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\':\\\\\\\\\\\\\\\\n-    app.run(debug=True, host=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'0.0.0.0\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\',port=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'8000\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\\\\\\\\\nindex 2b89fbc..29434e0 100644\\\\\\\\\\\\\\\\n--- a/src/facenet.py\\\\\\\\\\\\\\\\n+++ b/src/facenet.py\\\\\\\\\\\\\\\\n@@ -371,7 +371,7 @@ def load_model(model, input_map=None):\\\\\\\\\\\\\\\\n     model_exp = os.path.expanduser(model)\\\\\\\\\\\\\\\\n     if (os.path.isfile(model_exp)):\\\\\\\\\\\\\\\\n         print(\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'Model filename: %s\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' % model_exp)\\\\\\\\\\\\\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n+        with tf.io.gfile.GFile(model_exp,\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\') as f:\\\\\\\\\\\\\\\\n             graph_def = tf.compat.v1.GraphDef()\\\\\\\\\\\\\\\\n             graph_def.ParseFromString(f.read())\\\\\\\\\\\\\\\\n             tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')\\\\\\\\\\\\\\\\ndiff --git a/video/a b/video/a\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex 8b13789..0000000\\\\\\\\\\\\\\\\n--- a/video/a\\\\\\\\\\\\\\\\n+++ /dev/null\\\\\\\\\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\\\\\\\\\n-\\\\\\\\\\\\\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\\\\\\\\\\\\\ndeleted file mode 100644\\\\\\\\\\\\\\\\nindex a503c89..0000000\\\\\\\\\\\\\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\\\\\\\\\\\\\'\\\\\\\\n\\\\\\\\\\\\\\\\ No newline at end of file\\\\\\\\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\\\\\\\\nindex f7b2209..0d685b4 100644\\\\\\\\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\\\\\\\\ndiff --git a/src/__pycache__/facenet.cpython-37.pyc b/src/__pycache__/facenet.cpython-37.pyc\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 3b10b34..0000000\\\\\\\\nBinary files a/src/__pycache__/facenet.cpython-37.pyc and /dev/null differ\\\\\\\\ndiff --git a/src/__pycache__/facenet.cpython-38.pyc b/src/__pycache__/facenet.cpython-38.pyc\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 7d7d046..0000000\\\\\\\\nBinary files a/src/__pycache__/facenet.cpython-38.pyc and /dev/null differ\\\\\\\\ndiff --git a/src/align/__init__.py b/src/align/__init__.py\\\\\\\\nnew file mode 100644\\\\\\\\nindex 0000000..e69de29\\\\\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex b0bc64f..0000000\\\\\\\\n--- a/src/face_rec_cam.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,129 +0,0 @@\\\\\\\\n-import tensorflow as tf\\\\\\\\n-from imutils.video import VideoStream\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import imutils\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import math\\\\\\\\n-import pickle\\\\\\\\n-import align.detect_face\\\\\\\\n-import numpy as np\\\\\\\\n-import cv2\\\\\\\\n-import collections\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\n-\\\\\\\\n-def main():\\\\\\\\n-    parser = argparse.ArgumentParser()\\\\\\\\n-    parser.add_argument(\\\\\\\\\\\\\\\'--path\\\\\\\\\\\\\\\', help=\\\\\\\\\\\\\\\'Path of the video you want to test on.\\\\\\\\\\\\\\\', default=0)\\\\\\\\n-    args = parser.parse_args()\\\\\\\\n-\\\\\\\\n-    MINSIZE = 20\\\\\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\n-    FACTOR = 0.709\\\\\\\\n-    IMAGE_SIZE = 182\\\\\\\\n-    INPUT_IMAGE_SIZE = 160\\\\\\\\n-    CLASSIFIER_PATH = \\\\\\\\\\\\\\\'Models/facemodel.pkl\\\\\\\\\\\\\\\'\\\\\\\\n-    VIDEO_PATH = args.path\\\\\\\\n-    FACENET_MODEL_PATH = \\\\\\\\\\\\\\\'Models/20180402-114759.pb\\\\\\\\\\\\\\\'\\\\\\\\n-\\\\\\\\n-    # Load The Custom Classifier\\\\\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as file:\\\\\\\\n-        model, class_names = pickle.load(file)\\\\\\\\n-    print("Custom Classifier, Successfully loaded")\\\\\\\\n-\\\\\\\\n-    # Use TensorFlow 2.x features\\\\\\\\n-    physical_devices = tf.config.list_physical_devices(\\\\\\\\\\\\\\\'GPU\\\\\\\\\\\\\\\')\\\\\\\\n-    if physical_devices:\\\\\\\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\\\\\n-\\\\\\\\n-    # Create the session and load the model\\\\\\\\n-    with tf.Graph().as_default():\\\\\\\\n-        with tf.compat.v1.Session() as sess:\\\\\\\\n-            # Load the facenet model\\\\\\\\n-            print(\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\')\\\\\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n-\\\\\\\\n-            # Get input and output tensors\\\\\\\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n-            embedding_size = embeddings.shape[1]\\\\\\\\n-\\\\\\\\n-            # Initialize MTCNN\\\\\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\\\\\n-\\\\\\\\n-            people_detected = set()\\\\\\\\n-            person_detected = collections.Counter()\\\\\\\\n-\\\\\\\\n-            cap = VideoStream(src=0).start()\\\\\\\\n-\\\\\\\\n-            while True:\\\\\\\\n-                frame = cap.read()\\\\\\\\n-                frame = imutils.resize(frame, width=600)\\\\\\\\n-                frame = cv2.flip(frame, 1)\\\\\\\\n-\\\\\\\\n-                # Detect faces\\\\\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\n-\\\\\\\\n-                faces_found = bounding_boxes.shape[0]\\\\\\\\n-                try:\\\\\\\\n-                    if faces_found > 1:\\\\\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                    elif faces_found > 0:\\\\\\\\n-                        det = bounding_boxes[:, 0:4]\\\\\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\n-                        for i in range(faces_found):\\\\\\\\n-                            bb[i][0] = det[i][0]\\\\\\\\n-                            bb[i][1] = det[i][1]\\\\\\\\n-                            bb[i][2] = det[i][2]\\\\\\\\n-                            bb[i][3] = det[i][3]\\\\\\\\n-                            print(bb[i][3] - bb[i][1])\\\\\\\\n-                            print(frame.shape[0])\\\\\\\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\\\\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\n-\\\\\\\\n-                                # Run the model to get embeddings\\\\\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n-\\\\\\\\n-                                # Predict the class of the detected face\\\\\\\\n-                                predictions = model.predict_proba(emb_array)\\\\\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\\\\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\\\\\n-\\\\\\\\n-                                # If confidence is high enough, label the face\\\\\\\\n-                                if best_class_probabilities > 0.8:\\\\\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\\\\\n-                                    text_x = bb[i][0]\\\\\\\\n-                                    text_y = bb[i][3] + 20\\\\\\\\n-\\\\\\\\n-                                    name = class_names[best_class_indices[0]]\\\\\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\\\\\n-                                    person_detected[best_name] += 1\\\\\\\\n-                                else:\\\\\\\\n-                                    name = "Unknown"\\\\\\\\n-\\\\\\\\n-                except Exception as e:\\\\\\\\n-                    print(e)\\\\\\\\n-                    pass\\\\\\\\n-\\\\\\\\n-                cv2.imshow(\\\\\\\\\\\\\\\'Face Recognition\\\\\\\\\\\\\\\', frame)\\\\\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\\\\\\\\\'q\\\\\\\\\\\\\\\'):\\\\\\\\n-                    break\\\\\\\\n-\\\\\\\\n-            cap.release()\\\\\\\\n-            cv2.destroyAllWindows()\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    main()\\\\\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex aaf7193..0000000\\\\\\\\n--- a/src/face_rec_flask.py\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1,118 +0,0 @@\\\\\\\\n-from __future__ import absolute_import\\\\\\\\n-from __future__ import division\\\\\\\\n-from __future__ import print_function\\\\\\\\n-\\\\\\\\n-from flask import Flask\\\\\\\\n-from flask import render_template , request\\\\\\\\n-from flask_cors import CORS, cross_origin\\\\\\\\n-import tensorflow as tf\\\\\\\\n-import argparse\\\\\\\\n-import facenet\\\\\\\\n-import os\\\\\\\\n-import sys\\\\\\\\n-import math\\\\\\\\n-import pickle\\\\\\\\n-import align.detect_face\\\\\\\\n-import numpy as np\\\\\\\\n-import cv2\\\\\\\\n-import collections\\\\\\\\n-from sklearn.svm import SVC\\\\\\\\n-import base64\\\\\\\\n-\\\\\\\\n-MINSIZE = 20\\\\\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\\\\\n-FACTOR = 0.709\\\\\\\\n-IMAGE_SIZE = 182\\\\\\\\n-INPUT_IMAGE_SIZE = 160\\\\\\\\n-CLASSIFIER_PATH = \\\\\\\\\\\\\\\'../Models/facemodel.pkl\\\\\\\\\\\\\\\'\\\\\\\\n-FACENET_MODEL_PATH = \\\\\\\\\\\\\\\'../Models/20180402-114759.pb\\\\\\\\\\\\\\\'\\\\\\\\n-\\\\\\\\n-# Load The Custom Classifier\\\\\\\\n-with open(CLASSIFIER_PATH, \\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as file:\\\\\\\\n-    model, class_names = pickle.load(file)\\\\\\\\n-print("Custom Classifier, Successfully loaded")\\\\\\\\n-\\\\\\\\n-tf.Graph().as_default()\\\\\\\\n-\\\\\\\\n-# Cai dat GPU neu co\\\\\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-# Load the model\\\\\\\\n-print(\\\\\\\\\\\\\\\'Loading feature extraction model\\\\\\\\\\\\\\\')\\\\\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\\\\\n-\\\\\\\\n-# Get input and output tensors\\\\\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\\\\\n-embedding_size = embeddings.get_shape()[1]\\\\\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-app = Flask(__name__)\\\\\\\\n-CORS(app)\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\'/\\\\\\\\\\\\\\\')\\\\\\\\n-@cross_origin()\\\\\\\\n-def index():\\\\\\\\n-    return "OK!";\\\\\\\\n-\\\\\\\\n-@app.route(\\\\\\\\\\\\\\\'/recog\\\\\\\\\\\\\\\', methods=[\\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\'])\\\\\\\\n-@cross_origin()\\\\\\\\n-def upload_img_file():\\\\\\\\n-    if request.method == \\\\\\\\\\\\\\\'POST\\\\\\\\\\\\\\\':\\\\\\\\n-        # base 64\\\\\\\\n-        name="Unknown"\\\\\\\\n-        f = request.form.get(\\\\\\\\\\\\\\\'image\\\\\\\\\\\\\\\')\\\\\\\\n-        w = int(request.form.get(\\\\\\\\\\\\\\\'w\\\\\\\\\\\\\\\'))\\\\\\\\n-        h = int(request.form.get(\\\\\\\\\\\\\\\'h\\\\\\\\\\\\\\\'))\\\\\\\\n-\\\\\\\\n-        decoded_string = base64.b64decode(f)\\\\\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\\\\\n-        #frame = frame.reshape(w,h,3)\\\\\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\\\\\n-\\\\\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\\\\\n-\\\\\\\\n-        faces_found = bounding_boxes.shape[0]\\\\\\\\n-\\\\\\\\n-        if faces_found > 0:\\\\\\\\n-            det = bounding_boxes[:, 0:4]\\\\\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\\\\\n-            for i in range(faces_found):\\\\\\\\n-                bb[i][0] = det[i][0]\\\\\\\\n-                bb[i][1] = det[i][1]\\\\\\\\n-                bb[i][2] = det[i][2]\\\\\\\\n-                bb[i][3] = det[i][3]\\\\\\\\n-                cropped = frame\\\\\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\\\\\n-                scaled = facenet.prewhiten(scaled)\\\\\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\\\\\n-                predictions = model.predict_proba(emb_array)\\\\\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\\\\\n-                best_class_probabilities = predictions[\\\\\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\\\\\n-                best_name = class_names[best_class_indices[0]]\\\\\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\\\\\n-\\\\\\\\n-                if best_class_probabilities > 0.8:\\\\\\\\n-                    name = class_names[best_class_indices[0]]\\\\\\\\n-                else:\\\\\\\\n-                    name = "Unknown"\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-        return name;\\\\\\\\n-\\\\\\\\n-\\\\\\\\n-if __name__ == \\\\\\\\\\\\\\\'__main__\\\\\\\\\\\\\\\':\\\\\\\\n-    app.run(debug=True, host=\\\\\\\\\\\\\\\'0.0.0.0\\\\\\\\\\\\\\\',port=\\\\\\\\\\\\\\\'8000\\\\\\\\\\\\\\\')\\\\\\\\n-\\\\\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\\\\\nindex 2b89fbc..29434e0 100644\\\\\\\\n--- a/src/facenet.py\\\\\\\\n+++ b/src/facenet.py\\\\\\\\n@@ -371,7 +371,7 @@ def load_model(model, input_map=None):\\\\\\\\n     model_exp = os.path.expanduser(model)\\\\\\\\n     if (os.path.isfile(model_exp)):\\\\\\\\n         print(\\\\\\\\\\\\\\\'Model filename: %s\\\\\\\\\\\\\\\' % model_exp)\\\\\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as f:\\\\\\\\n+        with tf.io.gfile.GFile(model_exp,\\\\\\\\\\\\\\\'rb\\\\\\\\\\\\\\\') as f:\\\\\\\\n             graph_def = tf.compat.v1.GraphDef()\\\\\\\\n             graph_def.ParseFromString(f.read())\\\\\\\\n             tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\\\\\\\\\'\\\\\\\\\\\\\\\')\\\\\\\\ndiff --git a/video/a b/video/a\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex 8b13789..0000000\\\\\\\\n--- a/video/a\\\\\\\\n+++ /dev/null\\\\\\\\n@@ -1 +0,0 @@\\\\\\\\n-\\\\\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\\\\\ndeleted file mode 100644\\\\\\\\nindex a503c89..0000000\\\\\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\\\\\'\\\\n\\\\\\\\ No newline at end of file\\\\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\\\\nindex f7b2209..0d685b4 100644\\\\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\\\\ndiff --git a/src/__pycache__/facenet.cpython-37.pyc b/src/__pycache__/facenet.cpython-37.pyc\\\\ndeleted file mode 100644\\\\nindex 3b10b34..0000000\\\\nBinary files a/src/__pycache__/facenet.cpython-37.pyc and /dev/null differ\\\\ndiff --git a/src/__pycache__/facenet.cpython-38.pyc b/src/__pycache__/facenet.cpython-38.pyc\\\\ndeleted file mode 100644\\\\nindex 7d7d046..0000000\\\\nBinary files a/src/__pycache__/facenet.cpython-38.pyc and /dev/null differ\\\\ndiff --git a/src/align/__init__.py b/src/align/__init__.py\\\\nnew file mode 100644\\\\nindex 0000000..e69de29\\\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\\\ndeleted file mode 100644\\\\nindex b0bc64f..0000000\\\\n--- a/src/face_rec_cam.py\\\\n+++ /dev/null\\\\n@@ -1,129 +0,0 @@\\\\n-import tensorflow as tf\\\\n-from imutils.video import VideoStream\\\\n-import argparse\\\\n-import facenet\\\\n-import imutils\\\\n-import os\\\\n-import sys\\\\n-import math\\\\n-import pickle\\\\n-import align.detect_face\\\\n-import numpy as np\\\\n-import cv2\\\\n-import collections\\\\n-from sklearn.svm import SVC\\\\n-\\\\n-def main():\\\\n-    parser = argparse.ArgumentParser()\\\\n-    parser.add_argument(\\\\\\\'--path\\\\\\\', help=\\\\\\\'Path of the video you want to test on.\\\\\\\', default=0)\\\\n-    args = parser.parse_args()\\\\n-\\\\n-    MINSIZE = 20\\\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\\\n-    FACTOR = 0.709\\\\n-    IMAGE_SIZE = 182\\\\n-    INPUT_IMAGE_SIZE = 160\\\\n-    CLASSIFIER_PATH = \\\\\\\'Models/facemodel.pkl\\\\\\\'\\\\n-    VIDEO_PATH = args.path\\\\n-    FACENET_MODEL_PATH = \\\\\\\'Models/20180402-114759.pb\\\\\\\'\\\\n-\\\\n-    # Load The Custom Classifier\\\\n-    with open(CLASSIFIER_PATH, \\\\\\\'rb\\\\\\\') as file:\\\\n-        model, class_names = pickle.load(file)\\\\n-    print("Custom Classifier, Successfully loaded")\\\\n-\\\\n-    # Use TensorFlow 2.x features\\\\n-    physical_devices = tf.config.list_physical_devices(\\\\\\\'GPU\\\\\\\')\\\\n-    if physical_devices:\\\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\\\n-\\\\n-    # Create the session and load the model\\\\n-    with tf.Graph().as_default():\\\\n-        with tf.compat.v1.Session() as sess:\\\\n-            # Load the facenet model\\\\n-            print(\\\\\\\'Loading feature extraction model\\\\\\\')\\\\n-            facenet.load_model(FACENET_MODEL_PATH)\\\\n-\\\\n-            # Get input and output tensors\\\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n-            embedding_size = embeddings.shape[1]\\\\n-\\\\n-            # Initialize MTCNN\\\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\\\n-\\\\n-            people_detected = set()\\\\n-            person_detected = collections.Counter()\\\\n-\\\\n-            cap = VideoStream(src=0).start()\\\\n-\\\\n-            while True:\\\\n-                frame = cap.read()\\\\n-                frame = imutils.resize(frame, width=600)\\\\n-                frame = cv2.flip(frame, 1)\\\\n-\\\\n-                # Detect faces\\\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\n-\\\\n-                faces_found = bounding_boxes.shape[0]\\\\n-                try:\\\\n-                    if faces_found > 1:\\\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                    elif faces_found > 0:\\\\n-                        det = bounding_boxes[:, 0:4]\\\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\n-                        for i in range(faces_found):\\\\n-                            bb[i][0] = det[i][0]\\\\n-                            bb[i][1] = det[i][1]\\\\n-                            bb[i][2] = det[i][2]\\\\n-                            bb[i][3] = det[i][3]\\\\n-                            print(bb[i][3] - bb[i][1])\\\\n-                            print(frame.shape[0])\\\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\\\n-                                scaled = facenet.prewhiten(scaled)\\\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\n-\\\\n-                                # Run the model to get embeddings\\\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\n-\\\\n-                                # Predict the class of the detected face\\\\n-                                predictions = model.predict_proba(emb_array)\\\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\\\n-                                best_name = class_names[best_class_indices[0]]\\\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\\\n-\\\\n-                                # If confidence is high enough, label the face\\\\n-                                if best_class_probabilities > 0.8:\\\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\\\n-                                    text_x = bb[i][0]\\\\n-                                    text_y = bb[i][3] + 20\\\\n-\\\\n-                                    name = class_names[best_class_indices[0]]\\\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\\\n-                                    person_detected[best_name] += 1\\\\n-                                else:\\\\n-                                    name = "Unknown"\\\\n-\\\\n-                except Exception as e:\\\\n-                    print(e)\\\\n-                    pass\\\\n-\\\\n-                cv2.imshow(\\\\\\\'Face Recognition\\\\\\\', frame)\\\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\\\\\'q\\\\\\\'):\\\\n-                    break\\\\n-\\\\n-            cap.release()\\\\n-            cv2.destroyAllWindows()\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    main()\\\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\\\ndeleted file mode 100644\\\\nindex aaf7193..0000000\\\\n--- a/src/face_rec_flask.py\\\\n+++ /dev/null\\\\n@@ -1,118 +0,0 @@\\\\n-from __future__ import absolute_import\\\\n-from __future__ import division\\\\n-from __future__ import print_function\\\\n-\\\\n-from flask import Flask\\\\n-from flask import render_template , request\\\\n-from flask_cors import CORS, cross_origin\\\\n-import tensorflow as tf\\\\n-import argparse\\\\n-import facenet\\\\n-import os\\\\n-import sys\\\\n-import math\\\\n-import pickle\\\\n-import align.detect_face\\\\n-import numpy as np\\\\n-import cv2\\\\n-import collections\\\\n-from sklearn.svm import SVC\\\\n-import base64\\\\n-\\\\n-MINSIZE = 20\\\\n-THRESHOLD = [0.6, 0.7, 0.7]\\\\n-FACTOR = 0.709\\\\n-IMAGE_SIZE = 182\\\\n-INPUT_IMAGE_SIZE = 160\\\\n-CLASSIFIER_PATH = \\\\\\\'../Models/facemodel.pkl\\\\\\\'\\\\n-FACENET_MODEL_PATH = \\\\\\\'../Models/20180402-114759.pb\\\\\\\'\\\\n-\\\\n-# Load The Custom Classifier\\\\n-with open(CLASSIFIER_PATH, \\\\\\\'rb\\\\\\\') as file:\\\\n-    model, class_names = pickle.load(file)\\\\n-print("Custom Classifier, Successfully loaded")\\\\n-\\\\n-tf.Graph().as_default()\\\\n-\\\\n-# Cai dat GPU neu co\\\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\\\n-\\\\n-\\\\n-# Load the model\\\\n-print(\\\\\\\'Loading feature extraction model\\\\\\\')\\\\n-facenet.load_model(FACENET_MODEL_PATH)\\\\n-\\\\n-# Get input and output tensors\\\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\\\n-embedding_size = embeddings.get_shape()[1]\\\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\\\n-\\\\n-\\\\n-\\\\n-app = Flask(__name__)\\\\n-CORS(app)\\\\n-\\\\n-\\\\n-\\\\n-@app.route(\\\\\\\'/\\\\\\\')\\\\n-@cross_origin()\\\\n-def index():\\\\n-    return "OK!";\\\\n-\\\\n-@app.route(\\\\\\\'/recog\\\\\\\', methods=[\\\\\\\'POST\\\\\\\'])\\\\n-@cross_origin()\\\\n-def upload_img_file():\\\\n-    if request.method == \\\\\\\'POST\\\\\\\':\\\\n-        # base 64\\\\n-        name="Unknown"\\\\n-        f = request.form.get(\\\\\\\'image\\\\\\\')\\\\n-        w = int(request.form.get(\\\\\\\'w\\\\\\\'))\\\\n-        h = int(request.form.get(\\\\\\\'h\\\\\\\'))\\\\n-\\\\n-        decoded_string = base64.b64decode(f)\\\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\\\n-        #frame = frame.reshape(w,h,3)\\\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\\\n-\\\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\\\n-\\\\n-        faces_found = bounding_boxes.shape[0]\\\\n-\\\\n-        if faces_found > 0:\\\\n-            det = bounding_boxes[:, 0:4]\\\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\\\n-            for i in range(faces_found):\\\\n-                bb[i][0] = det[i][0]\\\\n-                bb[i][1] = det[i][1]\\\\n-                bb[i][2] = det[i][2]\\\\n-                bb[i][3] = det[i][3]\\\\n-                cropped = frame\\\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\\\n-                                    interpolation=cv2.INTER_CUBIC)\\\\n-                scaled = facenet.prewhiten(scaled)\\\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\\\n-                predictions = model.predict_proba(emb_array)\\\\n-                best_class_indices = np.argmax(predictions, axis=1)\\\\n-                best_class_probabilities = predictions[\\\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\\\n-                best_name = class_names[best_class_indices[0]]\\\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\\\n-\\\\n-                if best_class_probabilities > 0.8:\\\\n-                    name = class_names[best_class_indices[0]]\\\\n-                else:\\\\n-                    name = "Unknown"\\\\n-\\\\n-\\\\n-        return name;\\\\n-\\\\n-\\\\n-if __name__ == \\\\\\\'__main__\\\\\\\':\\\\n-    app.run(debug=True, host=\\\\\\\'0.0.0.0\\\\\\\',port=\\\\\\\'8000\\\\\\\')\\\\n-\\\\ndiff --git a/src/facenet.py b/src/facenet.py\\\\nindex 2b89fbc..29434e0 100644\\\\n--- a/src/facenet.py\\\\n+++ b/src/facenet.py\\\\n@@ -371,7 +371,7 @@ def load_model(model, input_map=None):\\\\n     model_exp = os.path.expanduser(model)\\\\n     if (os.path.isfile(model_exp)):\\\\n         print(\\\\\\\'Model filename: %s\\\\\\\' % model_exp)\\\\n-        with gfile.FastGFile(model_exp,\\\\\\\'rb\\\\\\\') as f:\\\\n+        with tf.io.gfile.GFile(model_exp,\\\\\\\'rb\\\\\\\') as f:\\\\n             graph_def = tf.compat.v1.GraphDef()\\\\n             graph_def.ParseFromString(f.read())\\\\n             tf.import_graph_def(graph_def, input_map=input_map, name=\\\\\\\'\\\\\\\')\\\\ndiff --git a/video/a b/video/a\\\\ndeleted file mode 100644\\\\nindex 8b13789..0000000\\\\n--- a/video/a\\\\n+++ /dev/null\\\\n@@ -1 +0,0 @@\\\\n-\\\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\\\ndeleted file mode 100644\\\\nindex a503c89..0000000\\\\nBinary files a/video/camtest.mp4 and /dev/null differ\\\'\\n\\\\ No newline at end of file\\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\\nindex f7b2209..0d685b4 100644\\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\\ndiff --git a/src/__pycache__/facenet.cpython-37.pyc b/src/__pycache__/facenet.cpython-37.pyc\\ndeleted file mode 100644\\nindex 3b10b34..0000000\\nBinary files a/src/__pycache__/facenet.cpython-37.pyc and /dev/null differ\\ndiff --git a/src/__pycache__/facenet.cpython-38.pyc b/src/__pycache__/facenet.cpython-38.pyc\\ndeleted file mode 100644\\nindex 7d7d046..0000000\\nBinary files a/src/__pycache__/facenet.cpython-38.pyc and /dev/null differ\\ndiff --git a/src/align/__init__.py b/src/align/__init__.py\\nnew file mode 100644\\nindex 0000000..e69de29\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\ndeleted file mode 100644\\nindex b0bc64f..0000000\\n--- a/src/face_rec_cam.py\\n+++ /dev/null\\n@@ -1,129 +0,0 @@\\n-import tensorflow as tf\\n-from imutils.video import VideoStream\\n-import argparse\\n-import facenet\\n-import imutils\\n-import os\\n-import sys\\n-import math\\n-import pickle\\n-import align.detect_face\\n-import numpy as np\\n-import cv2\\n-import collections\\n-from sklearn.svm import SVC\\n-\\n-def main():\\n-    parser = argparse.ArgumentParser()\\n-    parser.add_argument(\\\'--path\\\', help=\\\'Path of the video you want to test on.\\\', default=0)\\n-    args = parser.parse_args()\\n-\\n-    MINSIZE = 20\\n-    THRESHOLD = [0.6, 0.7, 0.7]\\n-    FACTOR = 0.709\\n-    IMAGE_SIZE = 182\\n-    INPUT_IMAGE_SIZE = 160\\n-    CLASSIFIER_PATH = \\\'Models/facemodel.pkl\\\'\\n-    VIDEO_PATH = args.path\\n-    FACENET_MODEL_PATH = \\\'Models/20180402-114759.pb\\\'\\n-\\n-    # Load The Custom Classifier\\n-    with open(CLASSIFIER_PATH, \\\'rb\\\') as file:\\n-        model, class_names = pickle.load(file)\\n-    print("Custom Classifier, Successfully loaded")\\n-\\n-    # Use TensorFlow 2.x features\\n-    physical_devices = tf.config.list_physical_devices(\\\'GPU\\\')\\n-    if physical_devices:\\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\\n-\\n-    # Create the session and load the model\\n-    with tf.Graph().as_default():\\n-        with tf.compat.v1.Session() as sess:\\n-            # Load the facenet model\\n-            print(\\\'Loading feature extraction model\\\')\\n-            facenet.load_model(FACENET_MODEL_PATH)\\n-\\n-            # Get input and output tensors\\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\n-            embedding_size = embeddings.shape[1]\\n-\\n-            # Initialize MTCNN\\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\\n-\\n-            people_detected = set()\\n-            person_detected = collections.Counter()\\n-\\n-            cap = VideoStream(src=0).start()\\n-\\n-            while True:\\n-                frame = cap.read()\\n-                frame = imutils.resize(frame, width=600)\\n-                frame = cv2.flip(frame, 1)\\n-\\n-                # Detect faces\\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\n-\\n-                faces_found = bounding_boxes.shape[0]\\n-                try:\\n-                    if faces_found > 1:\\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\\n-                    elif faces_found > 0:\\n-                        det = bounding_boxes[:, 0:4]\\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\\n-                        for i in range(faces_found):\\n-                            bb[i][0] = det[i][0]\\n-                            bb[i][1] = det[i][1]\\n-                            bb[i][2] = det[i][2]\\n-                            bb[i][3] = det[i][3]\\n-                            print(bb[i][3] - bb[i][1])\\n-                            print(frame.shape[0])\\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\\n-                                scaled = facenet.prewhiten(scaled)\\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\n-\\n-                                # Run the model to get embeddings\\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\n-\\n-                                # Predict the class of the detected face\\n-                                predictions = model.predict_proba(emb_array)\\n-                                best_class_indices = np.argmax(predictions, axis=1)\\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\\n-                                best_name = class_names[best_class_indices[0]]\\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\\n-\\n-                                # If confidence is high enough, label the face\\n-                                if best_class_probabilities > 0.8:\\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\\n-                                    text_x = bb[i][0]\\n-                                    text_y = bb[i][3] + 20\\n-\\n-                                    name = class_names[best_class_indices[0]]\\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\\n-                                    person_detected[best_name] += 1\\n-                                else:\\n-                                    name = "Unknown"\\n-\\n-                except Exception as e:\\n-                    print(e)\\n-                    pass\\n-\\n-                cv2.imshow(\\\'Face Recognition\\\', frame)\\n-                if cv2.waitKey(1) & 0xFF == ord(\\\'q\\\'):\\n-                    break\\n-\\n-            cap.release()\\n-            cv2.destroyAllWindows()\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    main()\\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\\ndeleted file mode 100644\\nindex aaf7193..0000000\\n--- a/src/face_rec_flask.py\\n+++ /dev/null\\n@@ -1,118 +0,0 @@\\n-from __future__ import absolute_import\\n-from __future__ import division\\n-from __future__ import print_function\\n-\\n-from flask import Flask\\n-from flask import render_template , request\\n-from flask_cors import CORS, cross_origin\\n-import tensorflow as tf\\n-import argparse\\n-import facenet\\n-import os\\n-import sys\\n-import math\\n-import pickle\\n-import align.detect_face\\n-import numpy as np\\n-import cv2\\n-import collections\\n-from sklearn.svm import SVC\\n-import base64\\n-\\n-MINSIZE = 20\\n-THRESHOLD = [0.6, 0.7, 0.7]\\n-FACTOR = 0.709\\n-IMAGE_SIZE = 182\\n-INPUT_IMAGE_SIZE = 160\\n-CLASSIFIER_PATH = \\\'../Models/facemodel.pkl\\\'\\n-FACENET_MODEL_PATH = \\\'../Models/20180402-114759.pb\\\'\\n-\\n-# Load The Custom Classifier\\n-with open(CLASSIFIER_PATH, \\\'rb\\\') as file:\\n-    model, class_names = pickle.load(file)\\n-print("Custom Classifier, Successfully loaded")\\n-\\n-tf.Graph().as_default()\\n-\\n-# Cai dat GPU neu co\\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\\n-\\n-\\n-# Load the model\\n-print(\\\'Loading feature extraction model\\\')\\n-facenet.load_model(FACENET_MODEL_PATH)\\n-\\n-# Get input and output tensors\\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n-embedding_size = embeddings.get_shape()[1]\\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\\n-\\n-\\n-\\n-app = Flask(__name__)\\n-CORS(app)\\n-\\n-\\n-\\n-@app.route(\\\'/\\\')\\n-@cross_origin()\\n-def index():\\n-    return "OK!";\\n-\\n-@app.route(\\\'/recog\\\', methods=[\\\'POST\\\'])\\n-@cross_origin()\\n-def upload_img_file():\\n-    if request.method == \\\'POST\\\':\\n-        # base 64\\n-        name="Unknown"\\n-        f = request.form.get(\\\'image\\\')\\n-        w = int(request.form.get(\\\'w\\\'))\\n-        h = int(request.form.get(\\\'h\\\'))\\n-\\n-        decoded_string = base64.b64decode(f)\\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\\n-        #frame = frame.reshape(w,h,3)\\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\\n-\\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\\n-\\n-        faces_found = bounding_boxes.shape[0]\\n-\\n-        if faces_found > 0:\\n-            det = bounding_boxes[:, 0:4]\\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\\n-            for i in range(faces_found):\\n-                bb[i][0] = det[i][0]\\n-                bb[i][1] = det[i][1]\\n-                bb[i][2] = det[i][2]\\n-                bb[i][3] = det[i][3]\\n-                cropped = frame\\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\\n-                                    interpolation=cv2.INTER_CUBIC)\\n-                scaled = facenet.prewhiten(scaled)\\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\\n-                predictions = model.predict_proba(emb_array)\\n-                best_class_indices = np.argmax(predictions, axis=1)\\n-                best_class_probabilities = predictions[\\n-                    np.arange(len(best_class_indices)), best_class_indices]\\n-                best_name = class_names[best_class_indices[0]]\\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\\n-\\n-                if best_class_probabilities > 0.8:\\n-                    name = class_names[best_class_indices[0]]\\n-                else:\\n-                    name = "Unknown"\\n-\\n-\\n-        return name;\\n-\\n-\\n-if __name__ == \\\'__main__\\\':\\n-    app.run(debug=True, host=\\\'0.0.0.0\\\',port=\\\'8000\\\')\\n-\\ndiff --git a/src/facenet.py b/src/facenet.py\\nindex 2b89fbc..29434e0 100644\\n--- a/src/facenet.py\\n+++ b/src/facenet.py\\n@@ -371,7 +371,7 @@ def load_model(model, input_map=None):\\n     model_exp = os.path.expanduser(model)\\n     if (os.path.isfile(model_exp)):\\n         print(\\\'Model filename: %s\\\' % model_exp)\\n-        with gfile.FastGFile(model_exp,\\\'rb\\\') as f:\\n+        with tf.io.gfile.GFile(model_exp,\\\'rb\\\') as f:\\n             graph_def = tf.compat.v1.GraphDef()\\n             graph_def.ParseFromString(f.read())\\n             tf.import_graph_def(graph_def, input_map=input_map, name=\\\'\\\')\\ndiff --git a/video/a b/video/a\\ndeleted file mode 100644\\nindex 8b13789..0000000\\n--- a/video/a\\n+++ /dev/null\\n@@ -1 +0,0 @@\\n-\\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\\ndeleted file mode 100644\\nindex a503c89..0000000\\nBinary files a/video/camtest.mp4 and /dev/null differ\'\n\\ No newline at end of file\ndiff --git a/Models/facemodel.pkl b/Models/facemodel.pkl\nindex f7b2209..0d685b4 100644\nBinary files a/Models/facemodel.pkl and b/Models/facemodel.pkl differ\ndiff --git a/src/__pycache__/facenet.cpython-37.pyc b/src/__pycache__/facenet.cpython-37.pyc\ndeleted file mode 100644\nindex 3b10b34..0000000\nBinary files a/src/__pycache__/facenet.cpython-37.pyc and /dev/null differ\ndiff --git a/src/__pycache__/facenet.cpython-38.pyc b/src/__pycache__/facenet.cpython-38.pyc\ndeleted file mode 100644\nindex 7d7d046..0000000\nBinary files a/src/__pycache__/facenet.cpython-38.pyc and /dev/null differ\ndiff --git a/src/align/__init__.py b/src/align/__init__.py\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\ndeleted file mode 100644\nindex b0bc64f..0000000\n--- a/src/face_rec_cam.py\n+++ /dev/null\n@@ -1,129 +0,0 @@\n-import tensorflow as tf\n-from imutils.video import VideoStream\n-import argparse\n-import facenet\n-import imutils\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n-    args = parser.parse_args()\n-\n-    MINSIZE = 20\n-    THRESHOLD = [0.6, 0.7, 0.7]\n-    FACTOR = 0.709\n-    IMAGE_SIZE = 182\n-    INPUT_IMAGE_SIZE = 160\n-    CLASSIFIER_PATH = \'Models/facemodel.pkl\'\n-    VIDEO_PATH = args.path\n-    FACENET_MODEL_PATH = \'Models/20180402-114759.pb\'\n-\n-    # Load The Custom Classifier\n-    with open(CLASSIFIER_PATH, \'rb\') as file:\n-        model, class_names = pickle.load(file)\n-    print("Custom Classifier, Successfully loaded")\n-\n-    # Use TensorFlow 2.x features\n-    physical_devices = tf.config.list_physical_devices(\'GPU\')\n-    if physical_devices:\n-        tf.config.set_logical_device_configuration(physical_devices[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n-\n-    # Create the session and load the model\n-    with tf.Graph().as_default():\n-        with tf.compat.v1.Session() as sess:\n-            # Load the facenet model\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(FACENET_MODEL_PATH)\n-\n-            # Get input and output tensors\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.shape[1]\n-\n-            # Initialize MTCNN\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n-\n-            people_detected = set()\n-            person_detected = collections.Counter()\n-\n-            cap = VideoStream(src=0).start()\n-\n-            while True:\n-                frame = cap.read()\n-                frame = imutils.resize(frame, width=600)\n-                frame = cv2.flip(frame, 1)\n-\n-                # Detect faces\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-                faces_found = bounding_boxes.shape[0]\n-                try:\n-                    if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (255, 255, 255), thickness=1, lineType=2)\n-                    elif faces_found > 0:\n-                        det = bounding_boxes[:, 0:4]\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\n-                        for i in range(faces_found):\n-                            bb[i][0] = det[i][0]\n-                            bb[i][1] = det[i][1]\n-                            bb[i][2] = det[i][2]\n-                            bb[i][3] = det[i][3]\n-                            print(bb[i][3] - bb[i][1])\n-                            print(frame.shape[0])\n-                            print((bb[i][3] - bb[i][1]) / frame.shape[0])\n-                            if (bb[i][3] - bb[i][1]) / frame.shape[0] > 0.25:\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC)\n-                                scaled = facenet.prewhiten(scaled)\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-\n-                                # Run the model to get embeddings\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-\n-                                # Predict the class of the detected face\n-                                predictions = model.predict_proba(emb_array)\n-                                best_class_indices = np.argmax(predictions, axis=1)\n-                                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-                                best_name = class_names[best_class_indices[0]]\n-                                print(f"Name: {best_name}, Probability: {best_class_probabilities}")\n-\n-                                # If confidence is high enough, label the face\n-                                if best_class_probabilities > 0.8:\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                                    text_x = bb[i][0]\n-                                    text_y = bb[i][3] + 20\n-\n-                                    name = class_names[best_class_indices[0]]\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    person_detected[best_name] += 1\n-                                else:\n-                                    name = "Unknown"\n-\n-                except Exception as e:\n-                    print(e)\n-                    pass\n-\n-                cv2.imshow(\'Face Recognition\', frame)\n-                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-                    break\n-\n-            cap.release()\n-            cv2.destroyAllWindows()\n-\n-if __name__ == \'__main__\':\n-    main()\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\ndeleted file mode 100644\nindex aaf7193..0000000\n--- a/src/face_rec_flask.py\n+++ /dev/null\n@@ -1,118 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from flask import Flask\n-from flask import render_template , request\n-from flask_cors import CORS, cross_origin\n-import tensorflow as tf\n-import argparse\n-import facenet\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-import base64\n-\n-MINSIZE = 20\n-THRESHOLD = [0.6, 0.7, 0.7]\n-FACTOR = 0.709\n-IMAGE_SIZE = 182\n-INPUT_IMAGE_SIZE = 160\n-CLASSIFIER_PATH = \'../Models/facemodel.pkl\'\n-FACENET_MODEL_PATH = \'../Models/20180402-114759.pb\'\n-\n-# Load The Custom Classifier\n-with open(CLASSIFIER_PATH, \'rb\') as file:\n-    model, class_names = pickle.load(file)\n-print("Custom Classifier, Successfully loaded")\n-\n-tf.Graph().as_default()\n-\n-# Cai dat GPU neu co\n-gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-\n-# Load the model\n-print(\'Loading feature extraction model\')\n-facenet.load_model(FACENET_MODEL_PATH)\n-\n-# Get input and output tensors\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-embedding_size = embeddings.get_shape()[1]\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\n-\n-\n-\n-app = Flask(__name__)\n-CORS(app)\n-\n-\n-\n-@app.route(\'/\')\n-@cross_origin()\n-def index():\n-    return "OK!";\n-\n-@app.route(\'/recog\', methods=[\'POST\'])\n-@cross_origin()\n-def upload_img_file():\n-    if request.method == \'POST\':\n-        # base 64\n-        name="Unknown"\n-        f = request.form.get(\'image\')\n-        w = int(request.form.get(\'w\'))\n-        h = int(request.form.get(\'h\'))\n-\n-        decoded_string = base64.b64decode(f)\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\n-        #frame = frame.reshape(w,h,3)\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\n-\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-        faces_found = bounding_boxes.shape[0]\n-\n-        if faces_found > 0:\n-            det = bounding_boxes[:, 0:4]\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\n-            for i in range(faces_found):\n-                bb[i][0] = det[i][0]\n-                bb[i][1] = det[i][1]\n-                bb[i][2] = det[i][2]\n-                bb[i][3] = det[i][3]\n-                cropped = frame\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                    interpolation=cv2.INTER_CUBIC)\n-                scaled = facenet.prewhiten(scaled)\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-                predictions = model.predict_proba(emb_array)\n-                best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[\n-                    np.arange(len(best_class_indices)), best_class_indices]\n-                best_name = class_names[best_class_indices[0]]\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-                if best_class_probabilities > 0.8:\n-                    name = class_names[best_class_indices[0]]\n-                else:\n-                    name = "Unknown"\n-\n-\n-        return name;\n-\n-\n-if __name__ == \'__main__\':\n-    app.run(debug=True, host=\'0.0.0.0\',port=\'8000\')\n-\ndiff --git a/src/facenet.py b/src/facenet.py\nindex 2b89fbc..29434e0 100644\n--- a/src/facenet.py\n+++ b/src/facenet.py\n@@ -371,7 +371,7 @@ def load_model(model, input_map=None):\n     model_exp = os.path.expanduser(model)\n     if (os.path.isfile(model_exp)):\n         print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n+        with tf.io.gfile.GFile(model_exp,\'rb\') as f:\n             graph_def = tf.compat.v1.GraphDef()\n             graph_def.ParseFromString(f.read())\n             tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\ndiff --git a/video/a b/video/a\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/video/a\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/video/camtest.mp4 b/video/camtest.mp4\ndeleted file mode 100644\nindex a503c89..0000000\nBinary files a/video/camtest.mp4 and /dev/null differ'